{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b22b3c7-9e1f-4c1e-99d7-d362adca8e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/FAC/FGSE/IDYST/tbeucler/default/freddy0218/miniconda3/envs/ships/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import netCDF4 as nf\n",
    "from netCDF4 import Dataset\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ast,gc\n",
    "from copy import deepcopy\n",
    "\n",
    "# Custom packages\n",
    "import read_config\n",
    "from util.data_process import read_vars, proc_dataset\n",
    "from util.models import performance_scores,train_baseline,causal_settings,train_PC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3705646-e7cf-49ad-9cc1-86706a6837a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read configuration file\n",
    "config_set = read_config.read_config()\n",
    "# Define Target\n",
    "if int(config_set['target_lag'])==4:\n",
    "    target='DELV24'\n",
    "elif int(config_set['target_lag'])==8:\n",
    "    target='DELV48'\n",
    "seed = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde9bba8-8049-40ec-8923-a846f53f4f68",
   "metadata": {},
   "source": [
    "# Create Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf48713-1e57-4515-b0f3-a92a80bae05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 623.51it/s]\n",
      "22it [00:00, 47.71it/s]\n",
      "22it [00:00, 120.92it/s]\n",
      "22it [00:00, 153.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process the filted TC list in the config file\n",
    "TC_tofilt_list = ast.literal_eval(config_set['TCfilt'])\n",
    "# Get the names of the remaining TCs\n",
    "filt_TClist = read_vars.remove_storms(trackpath=config_set['track_path'],basinID='NA',yearmin=int(config_set['start_year']),yearmax=int(config_set['end_year']),\n",
    "                                      remove_set=TC_tofilt_list)\n",
    "# Read saved SHIPS csvs\n",
    "storeSHIPS = read_vars.read_SHIPS_csv(startyear=int(config_set['start_year']),endyear=int(config_set['end_year']),vars_path=config_set['vars_path'],\n",
    "                                      filted_TCnames=filt_TClist,suffixlist=['newships_dev_POT'])\n",
    "# Read selected variables from the pandas dfs\n",
    "SHIPS_df = read_vars.create_SHIPS_df(startyear=int(config_set['start_year']),endyear=int(config_set['end_year']),SHIPSdict=storeSHIPS,\n",
    "                                     wantvarnames=config_set['SHIPSops_varname'],targetname=target,filted_TCnames=filt_TClist,\n",
    "                                     lagnum=int(config_set['target_lag'])\n",
    "                                    )\n",
    "# Add derived variables stored separately\n",
    "store_dfstorms_ships = read_vars.add_derive_df(startyear=int(config_set['start_year']),\n",
    "                                     endyear=int(config_set['end_year']),\n",
    "                                     SHIPSdict=SHIPS_df,\n",
    "                                     addfilepath='/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/SHIPS/ships_pkl/all_storms_ships23vars_obswmax.pkl',\n",
    "                                     addvarname=['pc20'],\n",
    "                                     filted_TCnames=filt_TClist,\n",
    "                                     lagnum=int(config_set['target_lag'])\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26063104-45c0-403f-bea3-b46126187307",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names=store_dfstorms_ships[2001]['ALLISON'].columns.values.tolist()\n",
    "\n",
    "TC_fulllist = {}\n",
    "for year in np.linspace(int(config_set['start_year']),int(config_set['end_year']),int(config_set['end_year'])-int(config_set['start_year'])+1):\n",
    "    temp = store_dfstorms_ships[year]\n",
    "    for ind,name in enumerate(temp.keys()):\n",
    "        TC_fulllist[str(int(year))+'_'+name] = temp[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d120fe6-543e-455e-84db-fabd8e0295bb",
   "metadata": {},
   "source": [
    "# ML-ready dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81038ac3-2f0a-41d6-b4e5-bcb718e8905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data with a 0.15 test, 0.15 valid split\n",
    "datastorer = proc_dataset.splitdata_handler(df=TC_fulllist,method='year',seed=seed,config=config_set,testyears=[2020,2021])\n",
    "\n",
    "# Remove empty storms in the data\n",
    "traincleaned = {key: datastorer['train'][key] for ind,key in enumerate(datastorer['train'].keys()) if datastorer['train'][key].shape[0]>0}\n",
    "validcleaned = {key: datastorer['valid'][key] for ind,key in enumerate(datastorer['valid'].keys()) if datastorer['valid'][key].shape[0]>0}\n",
    "testcleaned = {key: datastorer['test'][key] for ind,key in enumerate(datastorer['test'].keys()) if datastorer['test'][key].shape[0]>0}\n",
    "\n",
    "# Replace original training data with the cleaned version\n",
    "datastorer_n = deepcopy(datastorer)\n",
    "\n",
    "# Replace\n",
    "datastorer_n['train'] = traincleaned\n",
    "datastorer_n['valid'] = validcleaned\n",
    "datastorer_n['test'] = testcleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db17d004-ccf5-43ea-928d-8b826d7f8042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get smoothed MSLP data and argmin values\n",
    "smoothed_MSLP, MSLP_argmin = proc_dataset.proc_data(df=datastorer_n,\n",
    "                                                    seed=seed).smooth_and_minindices(varname='MSLP',sigma=3)\n",
    "# Aligned the inputs with the minimum SLP data\n",
    "aligned_train = proc_dataset.proc_data(df=datastorer_n,seed=seed).do_data_align(datastorer_n['train'],MSLP_argmin['train'],var_names)\n",
    "aligned_valid = proc_dataset.proc_data(df=datastorer_n,seed=seed).do_data_align(datastorer_n['valid'],MSLP_argmin['valid'],var_names)\n",
    "aligned_test = proc_dataset.proc_data(df=datastorer_n,seed=seed).do_data_align(datastorer_n['test'],MSLP_argmin['test'],var_names)\n",
    "\n",
    "# Combine different TCs into a long dataset\n",
    "X,y,size = proc_dataset.df_proc_separate(aligned_train,aligned_valid,aligned_test,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c03e33e4-fc50-4178-83f9-5b5cbec34cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mean and std of the training set for normalization\n",
    "trainmean,trainstd = X['train'].dropna().mean(axis=0),X['train'].dropna().std(axis=0)\n",
    "\n",
    "# Data normalization\n",
    "Xnorml = proc_dataset.normalized_TCs_handler(train=aligned_train,\n",
    "                                             valid=aligned_valid,\n",
    "                                             test=aligned_test,\n",
    "                                             trainmean=trainmean,\n",
    "                                             trainstd=trainstd,\n",
    "                                             dropcol=[target],\n",
    "                                             target=target\n",
    "                                            )\n",
    "var_names = Xnorml['train'][list(Xnorml['train'].keys())[0]].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319eafa7-ae3c-4637-80a4-a1d869b0f19c",
   "metadata": {},
   "source": [
    "# Causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9caec27f-0541-4f84-b7b9-06647927e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyships_lag = causal_settings.link_onlyships(numvar=aligned_train[list(aligned_train.keys())[0]].shape[1],\n",
    "                                               lag=int(config_set['target_lag']),\n",
    "                                               target_ind=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53e549fc-ba60-4b82-91b8-fcd34b7ca0f5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, None, None), 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/work/FAC/FGSE/IDYST/tbeucler/default/freddy0218/miniconda3/envs/ships/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:173\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '(slice(None, None, None), 2)' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m aligned_train\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m----> 2\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39mmasked_less(\u001b[43maligned_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m,\u001b[38;5;241m800\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#plt.ylim(0,1010)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/work/FAC/FGSE/IDYST/tbeucler/default/freddy0218/miniconda3/envs/ships/lib/python3.9/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/work/FAC/FGSE/IDYST/tbeucler/default/freddy0218/miniconda3/envs/ships/lib/python3.9/site-packages/pandas/core/indexes/base.py:3817\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m-> 3817\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_indexing_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3818\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/work/FAC/FGSE/IDYST/tbeucler/default/freddy0218/miniconda3/envs/ships/lib/python3.9/site-packages/pandas/core/indexes/base.py:6059\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   6055\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_indexing_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   6056\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(key):\n\u001b[1;32m   6057\u001b[0m         \u001b[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[1;32m   6058\u001b[0m         \u001b[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[0;32m-> 6059\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: (slice(None, None, None), 2)"
     ]
    }
   ],
   "source": [
    "for key in aligned_train.keys():\n",
    "    plt.plot(np.ma.masked_less(aligned_train[key][:,2],800))\n",
    "#plt.ylim(0,1010)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f506690-2cb6-4b71-a2ec-68fbdb44d47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:48<00:00,  1.52s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for pc_alpha in tqdm([0.0001, 0.00015 ,0.001,0.0015,0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,\n",
    "                      0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.86,0.88,0.9]):\n",
    "    Xnorml_c = {'train': {ind: np.asarray(Xnorml['train'][key].replace(np.nan,-999.0)) for ind,key in enumerate(Xnorml['train'].keys())},\n",
    "            'valid': {ind: np.asarray(Xnorml['valid'][key].replace(np.nan,-999.0)) for ind,key in enumerate(Xnorml['valid'].keys())},\n",
    "            'test': {ind: np.asarray(Xnorml['test'][key].replace(np.nan,-999.0)) for ind,key in enumerate(Xnorml['test'].keys())}\n",
    "           }\n",
    "    result = train_PC1.Pipeline(Xnorml_c['train'],\n",
    "                                pc_alpha,\n",
    "                                pc_type='run_pcstable',\n",
    "                                tau_min0=int(config_set['tau_min']),\n",
    "                                tau_max0=int(config_set['tau_max']),\n",
    "                                var_name=var_names,\n",
    "                                link_assumptions=onlyships_lag).run_tigramite()\n",
    "    del Xnorml_c\n",
    "    gc.collect()\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b02932-8dc3-49da-b3ee-0a1e3b1d86f7",
   "metadata": {},
   "source": [
    "# Performance Skill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f412558-201f-4175-be0b-d49e39d46970",
   "metadata": {},
   "source": [
    "## No causally-informed feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1392a6ae-e1db-4a2b-9ab2-d08b26dfa350",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ytrain = np.concatenate([np.asarray(Xnorml['train'][key].dropna()[target]) for key in Xnorml['train'].keys()],axis=0)\n",
    "Xtrain = np.concatenate([np.asarray(Xnorml['train'][key].dropna().drop(columns=[target])) for key in Xnorml['train'].keys()],axis=0)\n",
    "yvalid = np.concatenate([np.asarray(Xnorml['valid'][key].dropna()[target]) for key in Xnorml['valid'].keys()],axis=0)\n",
    "Xvalid = np.concatenate([np.asarray(Xnorml['valid'][key].dropna().drop(columns=[target])) for key in Xnorml['valid'].keys()],axis=0)\n",
    "ytest = np.concatenate([np.asarray(Xnorml['test'][key].dropna()[target]) for key in Xnorml['test'].keys()],axis=0)\n",
    "Xtest = np.concatenate([np.asarray(Xnorml['test'][key].dropna().drop(columns=[target])) for key in Xnorml['test'].keys()],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "517d9ed6-ef38-4f19-9cf2-bb40f44d2c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnorml_nocausal = {'train': Xtrain, 'valid': Xvalid, 'test': Xtest}\n",
    "y = {'train': ytrain, 'valid': yvalid, 'test': ytest}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4eefa5ad-e8f3-4a43-8c20-e2afed4dc245",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = train_baseline.train_baseline_MLR(Xnorml_nocausal,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14dc8eed-7a56-4a02-860c-fa9c547d7580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.39057866103395666, 0.32414988553072477, 0.3810791945004669)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLR_scoreboard = performance_scores.scoreboard(regr).store_scores(Xnorml_nocausal,y)\n",
    "MLR_scoreboard['train']['r2'],MLR_scoreboard['valid']['r2'],MLR_scoreboard['test']['r2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809d8992-9e5e-46c5-a48f-942bb0979ed8",
   "metadata": {},
   "source": [
    "## With causally-informed feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50bd52b4-ecd8-4936-bb1b-385a9b6b4e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_causal(PC1_results=None,Xnorml=None):\n",
    "    causal_predictor_list = [var_names[i] for i in [obj[0] for obj in PC1_results[0]]]\n",
    "    while target in causal_predictor_list: \n",
    "        causal_predictor_list.remove(target)\n",
    "        \n",
    "    Xtrain_causal = np.concatenate([np.asarray(Xnorml['train'][key].dropna()[causal_predictor_list]) for key in Xnorml['train'].keys()],axis=0)\n",
    "    Xvalid_causal = np.concatenate([np.asarray(Xnorml['valid'][key].dropna()[causal_predictor_list]) for key in Xnorml['valid'].keys()],axis=0)\n",
    "    Xtest_causal = np.concatenate([np.asarray(Xnorml['test'][key].dropna()[causal_predictor_list]) for key in Xnorml['test'].keys()],axis=0)\n",
    "    \n",
    "    Xnorml_causal = {'train': Xtrain_causal, 'valid': Xvalid_causal, 'test': Xtest_causal}\n",
    "    regr_causal = train_baseline.train_baseline_MLR(Xnorml_causal,y)\n",
    "    return performance_scores.scoreboard(regr_causal).store_scores(Xnorml_causal,y),Xnorml_causal,regr_causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9afd476-e3cb-4c33-bcdc-cbfa0a952ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores,Xs,regr = [],[],[]\n",
    "for obj in results:\n",
    "    score,X,regrz = benchmark_causal(PC1_results=obj,Xnorml=Xnorml)\n",
    "    scores.append(score)\n",
    "    Xs.append(X)\n",
    "    regr.append(regrz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcf6504b-7b44-4f04-8342-e19da58d8e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1021555358079308,\n",
       " 0.1021555358079308,\n",
       " 0.1021555358079308,\n",
       " 0.1021555358079308,\n",
       " 0.2452320855925929,\n",
       " 0.2452320855925929,\n",
       " 0.24405800371330577,\n",
       " 0.24379775741868182,\n",
       " 0.24379775741868182,\n",
       " 0.24379775741868182,\n",
       " 0.2446481135565478,\n",
       " 0.2709626786485583,\n",
       " 0.2709626786485583,\n",
       " 0.268247385815237,\n",
       " 0.2853524218899478,\n",
       " 0.28628801845663976,\n",
       " 0.28628801845663976,\n",
       " 0.28628801845663976,\n",
       " 0.28628801845663976,\n",
       " 0.28628801845663976,\n",
       " 0.28628801845663976,\n",
       " 0.28628801845663976,\n",
       " 0.28863877427561013,\n",
       " 0.28863877427561013,\n",
       " 0.28863877427561013,\n",
       " 0.28863877427561013,\n",
       " 0.2744416432922989,\n",
       " 0.28155526779540596,\n",
       " 0.28448719302397096,\n",
       " 0.28448719302397096,\n",
       " 0.28448719302397096,\n",
       " 0.292464581726671]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[obj['valid']['r2'] for obj in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a01c727a-76aa-48ad-93ff-d5a209f2d033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.009977204607032775,\n",
       " 0.009977204607032775,\n",
       " 0.009977204607032775,\n",
       " 0.009977204607032775,\n",
       " 0.2716924929790153,\n",
       " 0.2716924929790153,\n",
       " 0.2750141812017797,\n",
       " 0.27450444848794664,\n",
       " 0.27450444848794664,\n",
       " 0.27450444848794664,\n",
       " 0.27774375521667305,\n",
       " 0.27871858308850184,\n",
       " 0.27871858308850184,\n",
       " 0.28418523034269094,\n",
       " 0.29524352448890256,\n",
       " 0.2973222397860653,\n",
       " 0.2973222397860653,\n",
       " 0.2973222397860653,\n",
       " 0.2973222397860653,\n",
       " 0.2973222397860653,\n",
       " 0.2973222397860653,\n",
       " 0.2973222397860653,\n",
       " 0.2944053005698549,\n",
       " 0.2944053005698549,\n",
       " 0.2944053005698549,\n",
       " 0.2944053005698549,\n",
       " 0.28052070333758905,\n",
       " 0.2981138735772464,\n",
       " 0.30615659889086966,\n",
       " 0.30615659889086966,\n",
       " 0.30615659889086966,\n",
       " 0.3178415691993145]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[obj['test']['r2'] for obj in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80e555-fec8-4a7a-82fa-69e568031559",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
