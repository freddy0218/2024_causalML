{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a574145-b906-4f60-bf64-78c37bea1bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import netCDF4 as nf\n",
    "from netCDF4 import Dataset\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "from copy import deepcopy\n",
    "\n",
    "# Custom packages\n",
    "import read_config\n",
    "from util.data_process import read_vars, proc_dataset\n",
    "from util.models import performance_scores,train_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01913bbd-3794-4dbe-9aac-19564ad3162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read configuration file\n",
    "config_set = read_config.read_config()\n",
    "# Define Target\n",
    "if int(config_set['target_lag'])==4:\n",
    "    target='delv24'\n",
    "\n",
    "seed=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e5414e9-cce6-416d-b329-dc03f4f29b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 663.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process the filted TC list in the config file\n",
    "TC_tofilt_list = ast.literal_eval(config_set['TCfilt'])\n",
    "# Get the names of the remaining TCs\n",
    "filt_TClist = read_vars.remove_storms(trackpath=config_set['track_path'],basinID='NA',yearmin=int(config_set['start_year']),yearmax=int(config_set['end_year']),\n",
    "                                      remove_set=TC_tofilt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca8e1a3a-9e31-47bd-993b-d413b43be2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:02, 10.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process the variable names to remove in the config file\n",
    "ERA5_dropvarname = ast.literal_eval(config_set['ERA5_dropvarname'])\n",
    "# Read saved ERA5 csvs\n",
    "storeERA5 = read_vars.read_ERA5_csv(startyear=int(config_set['start_year']),endyear=int(config_set['end_year']),vars_path=config_set['vars_path'],\n",
    "                                      filted_TCnames=filt_TClist,suffixlist=['obsw_dwmax','tigramite_6hr'],era5_dropvar=ERA5_dropvarname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c72c5b47-7eb1-48cb-a601-6d4d79c209a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:10,  2.01it/s]\n"
     ]
    }
   ],
   "source": [
    "storeERA5_PRIMED = read_vars.read_TCPRIMED_df(startyear=int(config_set['start_year']),\n",
    "                                              endyear=int(config_set['end_year']),\n",
    "                                              ERA5dict=storeERA5,\n",
    "                                              filted_TCnames=filt_TClist,\n",
    "                                              PRIMEDpath=config_set['PRIMED_path'],\n",
    "                                              PRIMEDlevels=ast.literal_eval(config_set['PRIMED_levels'])\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f3fd1ee-9d5b-44d5-bff9-fe159c6b65a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:00, 61.65it/s]\n"
     ]
    }
   ],
   "source": [
    "storeERA5_all = read_vars.create_ERA5_df(startyear=int(config_set['start_year']),\n",
    "                                         endyear=int(config_set['end_year']),\n",
    "                                         ERA5SPS_path=config_set['ERA5SPS_path'],\n",
    "                                         ERA5SPS_suffix='all_storms_ships23vars_obswmax.pkl',\n",
    "                                         ERA5dict=storeERA5_PRIMED,\n",
    "                                         wantvarnames=ast.literal_eval(config_set['ERA5SPS_varname']),\n",
    "                                         targetname=target,\n",
    "                                         filted_TCnames=filt_TClist,\n",
    "                                         lagnum=int(config_set['target_lag']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d343579f-8032-4dff-87aa-a4d6f4a6d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names=storeERA5_all[2001]['ALLISON'].columns.values.tolist()\n",
    "    \n",
    "TC_fulllist = {}\n",
    "for year in np.linspace(int(config_set['start_year']),int(config_set['end_year']),int(config_set['end_year'])-int(config_set['start_year'])+1):\n",
    "    temp = storeERA5_all[year]\n",
    "    for ind,name in enumerate(temp.keys()):\n",
    "        TC_fulllist[str(int(year))+'_'+name] = temp[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ebedf8b-8a7b-4c7b-814f-6191ed33ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------\n",
    "# ML-ready dataset\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "# Split data with a 0.15 test, 0.15 valid split\n",
    "datastorer = proc_dataset.splitdata_handler(df=TC_fulllist,\n",
    "                                            method='year',\n",
    "                                            seed=seed,\n",
    "                                            config=config_set,\n",
    "                                            testyears=[2020,2021]\n",
    "                                           )\n",
    "# Remove empty storms in the data\n",
    "traincleaned = {key: datastorer['train'][key] for ind,key in enumerate(datastorer['train'].keys()) if datastorer['train'][key].shape[0]>0}\n",
    "validcleaned = {key: datastorer['valid'][key] for ind,key in enumerate(datastorer['valid'].keys()) if datastorer['valid'][key].shape[0]>0}\n",
    "testcleaned = {key: datastorer['test'][key] for ind,key in enumerate(datastorer['test'].keys()) if datastorer['test'][key].shape[0]>0}\n",
    "    \n",
    "# Replace original training data with the cleaned version\n",
    "datastorer_n = deepcopy(datastorer)\n",
    "    \n",
    "# Replace\n",
    "datastorer_n['train'] = traincleaned\n",
    "datastorer_n['valid'] = validcleaned\n",
    "datastorer_n['test'] = testcleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7266a8f3-47f0-458a-9f02-78947c5c2f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get smoothed MSLP data and argmin values\n",
    "smoothed_MSLP, MSLP_argmin = proc_dataset.proc_data(df=datastorer_n,seed=seed).smooth_and_minindices(varname='pmin',sigma=3)\n",
    "# Aligned the inputs with the minimum SLP data\n",
    "aligned_train = proc_dataset.proc_data(df=datastorer_n,seed=seed).do_data_align(datastorer_n['train'],MSLP_argmin['train'],var_names)\n",
    "aligned_valid = proc_dataset.proc_data(df=datastorer_n,seed=seed).do_data_align(datastorer_n['valid'],MSLP_argmin['valid'],var_names)\n",
    "aligned_test = proc_dataset.proc_data(df=datastorer_n,seed=seed).do_data_align(datastorer_n['test'],MSLP_argmin['test'],var_names)\n",
    "    \n",
    "# Combine different TCs into a long dataset\n",
    "X,y,size = proc_dataset.df_proc_separate(aligned_train,aligned_valid,aligned_test,target)\n",
    "\n",
    "# Find the mean and std of the training set for normalization\n",
    "trainmean,trainstd = X['train'].dropna().mean(axis=0),X['train'].dropna().std(axis=0)\n",
    "\n",
    "# Data normalization\n",
    "Xnorml = proc_dataset.normalized_TCs_handler(train=aligned_train,\n",
    "                                             valid=aligned_valid,\n",
    "                                             test=aligned_test,\n",
    "                                             trainmean=trainmean,\n",
    "                                             trainstd=trainstd,\n",
    "                                             dropcol=[target],\n",
    "                                             target=target\n",
    "                                            )\n",
    "var_names = Xnorml['train'][list(Xnorml['train'].keys())[0]].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae59bb8-6e11-4196-bc00-7f17b135d6da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
