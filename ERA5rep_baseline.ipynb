{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a574145-b906-4f60-bf64-78c37bea1bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/FAC/FGSE/IDYST/tbeucler/default/freddy0218/miniconda3/envs/ships/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import netCDF4 as nf\n",
    "from netCDF4 import Dataset\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ast,gc\n",
    "from copy import deepcopy\n",
    "\n",
    "# Custom packages\n",
    "import read_config\n",
    "from util.data_process import read_vars, proc_dataset\n",
    "from util.models import performance_scores,train_baseline,causal_settings,train_PC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01913bbd-3794-4dbe-9aac-19564ad3162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read configuration file\n",
    "config_set = read_config.read_config()\n",
    "# Define Target\n",
    "if int(config_set['target_lag'])==4:\n",
    "    target='delv24'\n",
    "\n",
    "seed=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25d12fed-80a9-47b4-8e12-96a3b49c175f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NA'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_set['basin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e5414e9-cce6-416d-b329-dc03f4f29b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 511.99it/s]\n"
     ]
    }
   ],
   "source": [
    "for seed in seeds:\n",
    "    # Process the filted TC list in the config file\n",
    "    TC_tofilt_list = ast.literal_eval(config_set['TCfilt'])\n",
    "    # Get the names of the remaining TCs\n",
    "    filt_TClist = read_vars.remove_storms(trackpath=config_set['track_path'],\n",
    "                                          basinID=config_set['basin'],\n",
    "                                          yearmin=int(config_set['start_year']),\n",
    "                                          yearmax=int(config_set['end_year']),\n",
    "                                          remove_set=TC_tofilt_list\n",
    "                                         )\n",
    "    # Process the variable names to remove in the config file\n",
    "    ERA5_dropvarname = ast.literal_eval(config_set['ERA5_dropvarname'])\n",
    "    # Read saved ERA5 csvs]\n",
    "    storeERA5 = read_vars.read_ERA5_csv(startyear=int(config_set['start_year']),\n",
    "                                        endyear=int(config_set['end_year']),\n",
    "                                        vars_path=config_set['vars_path'],\n",
    "                                        filted_TCnames=filt_TClist,\n",
    "                                        suffixlist=['obsw_dwmax','tigramite_6hr'],\n",
    "                                        era5_dropvar=ERA5_dropvarname\n",
    "                                       )\n",
    "    \n",
    "    storeERA5_PRIMED = read_vars.read_TCPRIMED_df(startyear=int(config_set['start_year']),\n",
    "                                                  endyear=int(config_set['end_year']),\n",
    "                                                  ERA5dict=storeERA5,\n",
    "                                                  filted_TCnames=filt_TClist,\n",
    "                                                  PRIMEDpath=config_set['PRIMED_path'],\n",
    "                                                  PRIMEDlevels=ast.literal_eval(config_set['PRIMED_levels'])\n",
    "                                                 )\n",
    "    \n",
    "    storeERA5_all = read_vars.create_ERA5_df(startyear=int(config_set['start_year']),\n",
    "                                             endyear=int(config_set['end_year']),\n",
    "                                             ERA5SPS_path=config_set['ERA5SPS_path'],\n",
    "                                             ERA5SPS_suffix='all_storms_ships23vars_obswmax.pkl',\n",
    "                                             ERA5dict=storeERA5_PRIMED,\n",
    "                                             wantvarnames=ast.literal_eval(config_set['ERA5SPS_varname']),\n",
    "                                             targetname=target,\n",
    "                                             filted_TCnames=filt_TClist,\n",
    "                                             lagnum=int(config_set['target_lag'])\n",
    "                                            )\n",
    "\n",
    "    var_names=storeERA5_all[2001]['ALLISON'].columns.values.tolist()\n",
    "    \n",
    "    TC_fulllist = {}\n",
    "    for year in np.linspace(int(config_set['start_year']),int(config_set['end_year']),int(config_set['end_year'])-int(config_set['start_year'])+1):\n",
    "        temp = storeERA5_all[year]\n",
    "        for ind,name in enumerate(temp.keys()):\n",
    "            TC_fulllist[str(int(year))+'_'+name] = temp[name]\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "    # ML-ready dataset\n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "    # Split data with a 0.15 test, 0.15 valid split\n",
    "    datastorer = proc_dataset.splitdata_handler(df=TC_fulllist,\n",
    "                                                method='year',\n",
    "                                                seed=seed,\n",
    "                                                config=config_set,\n",
    "                                                testyears=[2020,2021]\n",
    "                                               )\n",
    "    # Remove empty storms in the data\n",
    "    traincleaned = {key: datastorer['train'][key] for ind,key in enumerate(datastorer['train'].keys()) if datastorer['train'][key].shape[0]>0}\n",
    "    validcleaned = {key: datastorer['valid'][key] for ind,key in enumerate(datastorer['valid'].keys()) if datastorer['valid'][key].shape[0]>0}\n",
    "    testcleaned = {key: datastorer['test'][key] for ind,key in enumerate(datastorer['test'].keys()) if datastorer['test'][key].shape[0]>0}\n",
    "    \n",
    "    # Replace original training data with the cleaned version\n",
    "    datastorer_n = deepcopy(datastorer)\n",
    "    \n",
    "    # Replace\n",
    "    datastorer_n['train'] = traincleaned\n",
    "    datastorer_n['valid'] = validcleaned\n",
    "    datastorer_n['test'] = testcleaned\n",
    "\n",
    "    # Get smoothed MSLP data and argmin values\n",
    "    smoothed_MSLP, MSLP_argmin = proc_dataset.proc_data(df=datastorer_n,seed=seed).smooth_and_minindices(varname='pmin',sigma=3)\n",
    "    # Aligned the inputs with the minimum SLP data\n",
    "    aligned_train = proc_dataset.proc_data(df=datastorer_n,seed=seed).do_data_align(datastorer_n['train'],MSLP_argmin['train'],var_names)\n",
    "    aligned_valid = proc_dataset.proc_data(df=datastorer_n,seed=seed).do_data_align(datastorer_n['valid'],MSLP_argmin['valid'],var_names)\n",
    "    aligned_test = proc_dataset.proc_data(df=datastorer_n,seed=seed).do_data_align(datastorer_n['test'],MSLP_argmin['test'],var_names)\n",
    "    \n",
    "    # Combine different TCs into a long dataset\n",
    "    X,y,size = proc_dataset.df_proc_separate(aligned_train,aligned_valid,aligned_test,target)\n",
    "\n",
    "    # Find the mean and std of the training set for normalization\n",
    "    trainmean,trainstd = X['train'].dropna().mean(axis=0),X['train'].dropna().std(axis=0)\n",
    "\n",
    "    # Data normalization\n",
    "    Xnorml = proc_dataset.normalized_TCs_handler(train=aligned_train,\n",
    "                                                 valid=aligned_valid,\n",
    "                                                 test=aligned_test,\n",
    "                                                 trainmean=trainmean,\n",
    "                                                 trainstd=trainstd,\n",
    "                                                 dropcol=[target],\n",
    "                                                 target=target\n",
    "                                                )\n",
    "    var_names = Xnorml['train'][list(Xnorml['train'].keys())[0]].columns\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "    # Causal\n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "    onlyships_lag = causal_settings.link_shipsera5(numvar=aligned_train[list(aligned_train.keys())[0]].shape[1],\n",
    "                                                   lag=int(config_set['target_lag']),\n",
    "                                                   target_ind=[0],\n",
    "                                                   ships_ind=23\n",
    "                                                  )\n",
    "\n",
    "    results = []\n",
    "    for pc_alpha in tqdm([0.75]):\n",
    "        Xnorml_c = {'train': {ind: np.asarray(Xnorml['train'][key].replace(np.nan,-999.0)) for ind,key in enumerate(Xnorml['train'].keys())},\n",
    "                    'valid': {ind: np.asarray(Xnorml['valid'][key].replace(np.nan,-999.0)) for ind,key in enumerate(Xnorml['valid'].keys())},\n",
    "                    'test': {ind: np.asarray(Xnorml['test'][key].replace(np.nan,-999.0)) for ind,key in enumerate(Xnorml['test'].keys())}\n",
    "                   }\n",
    "        result = train_PC1.Pipeline(Xnorml_c['train'],\n",
    "                                    pc_alpha,\n",
    "                                    pc_type='run_pcstable',\n",
    "                                    tau_min0=int(config_set['tau_min']),\n",
    "                                    tau_max0=int(config_set['tau_max']),\n",
    "                                    var_name=var_names,\n",
    "                                    link_assumptions=onlyships_lag).run_tigramite()\n",
    "        del Xnorml_c\n",
    "        gc.collect()\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca8e1a3a-9e31-47bd-993b-d413b43be2f8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:02,  9.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process the variable names to remove in the config file\n",
    "ERA5_dropvarname = ast.literal_eval(config_set['ERA5_dropvarname'])\n",
    "# Read saved ERA5 csvs\n",
    "storeERA5 = read_vars.read_ERA5_csv(startyear=int(config_set['start_year']),endyear=int(config_set['end_year']),vars_path=config_set['vars_path'],\n",
    "                                      filted_TCnames=filt_TClist,suffixlist=['obsw_dwmax','tigramite_6hr'],era5_dropvar=ERA5_dropvarname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c72c5b47-7eb1-48cb-a601-6d4d79c209a9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:13,  1.57it/s]\n"
     ]
    }
   ],
   "source": [
    "storeERA5_PRIMED = read_vars.read_TCPRIMED_df(startyear=int(config_set['start_year']),\n",
    "                                              endyear=int(config_set['end_year']),\n",
    "                                              ERA5dict=storeERA5,\n",
    "                                              filted_TCnames=filt_TClist,\n",
    "                                              PRIMEDpath=config_set['PRIMED_path'],\n",
    "                                              PRIMEDlevels=ast.literal_eval(config_set['PRIMED_levels'])\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f3fd1ee-9d5b-44d5-bff9-fe159c6b65a5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:00, 57.70it/s]\n"
     ]
    }
   ],
   "source": [
    "storeERA5_all = read_vars.create_ERA5_df(startyear=int(config_set['start_year']),\n",
    "                                         endyear=int(config_set['end_year']),\n",
    "                                         ERA5SPS_path=config_set['ERA5SPS_path'],\n",
    "                                         ERA5SPS_suffix='all_storms_ships23vars_obswmax.pkl',\n",
    "                                         ERA5dict=storeERA5_PRIMED,\n",
    "                                         wantvarnames=ast.literal_eval(config_set['ERA5SPS_varname']),\n",
    "                                         targetname=target,\n",
    "                                         filted_TCnames=filt_TClist,\n",
    "                                         lagnum=int(config_set['target_lag']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d343579f-8032-4dff-87aa-a4d6f4a6d082",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "var_names=storeERA5_all[2001]['ALLISON'].columns.values.tolist()\n",
    "    \n",
    "TC_fulllist = {}\n",
    "for year in np.linspace(int(config_set['start_year']),int(config_set['end_year']),int(config_set['end_year'])-int(config_set['start_year'])+1):\n",
    "    temp = storeERA5_all[year]\n",
    "    for ind,name in enumerate(temp.keys()):\n",
    "        TC_fulllist[str(int(year))+'_'+name] = temp[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ebedf8b-8a7b-4c7b-814f-6191ed33ef80",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------\n",
    "# ML-ready dataset\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "# Split data with a 0.15 test, 0.15 valid split\n",
    "datastorer = proc_dataset.splitdata_handler(df=TC_fulllist,\n",
    "                                            method='year',\n",
    "                                            seed=seed,\n",
    "                                            config=config_set,\n",
    "                                            testyears=[2020,2021]\n",
    "                                           )\n",
    "# Remove empty storms in the data\n",
    "traincleaned = {key: datastorer['train'][key] for ind,key in enumerate(datastorer['train'].keys()) if datastorer['train'][key].shape[0]>0}\n",
    "validcleaned = {key: datastorer['valid'][key] for ind,key in enumerate(datastorer['valid'].keys()) if datastorer['valid'][key].shape[0]>0}\n",
    "testcleaned = {key: datastorer['test'][key] for ind,key in enumerate(datastorer['test'].keys()) if datastorer['test'][key].shape[0]>0}\n",
    "    \n",
    "# Replace original training data with the cleaned version\n",
    "datastorer_n = deepcopy(datastorer)\n",
    "    \n",
    "# Replace\n",
    "datastorer_n['train'] = traincleaned\n",
    "datastorer_n['valid'] = validcleaned\n",
    "datastorer_n['test'] = testcleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7266a8f3-47f0-458a-9f02-78947c5c2f82",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Get smoothed MSLP data and argmin values\n",
    "smoothed_MSLP, MSLP_argmin = proc_dataset.proc_data(df=datastorer_n,seed=seed).smooth_and_minindices(varname='pmin',sigma=3)\n",
    "# Aligned the inputs with the minimum SLP data\n",
    "aligned_train = proc_dataset.proc_data(df=datastorer_n,seed=seed).do_data_align(datastorer_n['train'],MSLP_argmin['train'],var_names)\n",
    "aligned_valid = proc_dataset.proc_data(df=datastorer_n,seed=seed).do_data_align(datastorer_n['valid'],MSLP_argmin['valid'],var_names)\n",
    "aligned_test = proc_dataset.proc_data(df=datastorer_n,seed=seed).do_data_align(datastorer_n['test'],MSLP_argmin['test'],var_names)\n",
    "    \n",
    "# Combine different TCs into a long dataset\n",
    "X,y,size = proc_dataset.df_proc_separate(aligned_train,aligned_valid,aligned_test,target)\n",
    "\n",
    "# Find the mean and std of the training set for normalization\n",
    "trainmean,trainstd = X['train'].dropna().mean(axis=0),X['train'].dropna().std(axis=0)\n",
    "\n",
    "# Data normalization\n",
    "Xnorml = proc_dataset.normalized_TCs_handler(train=aligned_train,\n",
    "                                             valid=aligned_valid,\n",
    "                                             test=aligned_test,\n",
    "                                             trainmean=trainmean,\n",
    "                                             trainstd=trainstd,\n",
    "                                             dropcol=[target],\n",
    "                                             target=target\n",
    "                                            )\n",
    "var_names = Xnorml['train'][list(Xnorml['train'].keys())[0]].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dae59bb8-6e11-4196-bc00-7f17b135d6da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------\n",
    "# Causal\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "onlyships_lag = causal_settings.link_shipsera5(numvar=aligned_train[list(aligned_train.keys())[0]].shape[1],\n",
    "                                               lag=int(config_set['target_lag']),\n",
    "                                               target_ind=[0],\n",
    "                                               ships_ind=23\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5e1d552f-d1d5-4fa1-a891-fc472d474ae7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:59<00:00, 59.09s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for pc_alpha in tqdm([0.75]):\n",
    "    Xnorml_c = {'train': {ind: np.asarray(Xnorml['train'][key].replace(np.nan,-999.0)) for ind,key in enumerate(Xnorml['train'].keys())},\n",
    "                'valid': {ind: np.asarray(Xnorml['valid'][key].replace(np.nan,-999.0)) for ind,key in enumerate(Xnorml['valid'].keys())},\n",
    "                'test': {ind: np.asarray(Xnorml['test'][key].replace(np.nan,-999.0)) for ind,key in enumerate(Xnorml['test'].keys())}\n",
    "               }\n",
    "    result = train_PC1.Pipeline(Xnorml_c['train'],\n",
    "                                pc_alpha,\n",
    "                                pc_type='run_pcstable',\n",
    "                                tau_min0=int(config_set['tau_min']),\n",
    "                                tau_max0=int(config_set['tau_max']),\n",
    "                                var_name=var_names,\n",
    "                                link_assumptions=onlyships_lag).run_tigramite()\n",
    "    del Xnorml_c\n",
    "    gc.collect()\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31b01fbe-a0e6-48da-8242-a48f2538769d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, -4),\n",
       " (191, -4),\n",
       " (0, -4),\n",
       " (244, -4),\n",
       " (19, -4),\n",
       " (11, -4),\n",
       " (169, -4),\n",
       " (16, -4),\n",
       " (10, -4),\n",
       " (149, -4),\n",
       " (50, -4),\n",
       " (241, -4),\n",
       " (92, -4),\n",
       " (39, -4),\n",
       " (175, -4),\n",
       " (128, -4),\n",
       " (12, -4),\n",
       " (138, -4),\n",
       " (31, -4),\n",
       " (67, -4),\n",
       " (28, -4),\n",
       " (21, -4),\n",
       " (9, -4),\n",
       " (184, -4),\n",
       " (204, -4),\n",
       " (155, -4),\n",
       " (131, -4),\n",
       " (164, -4),\n",
       " (152, -4),\n",
       " (23, -4),\n",
       " (133, -4),\n",
       " (73, -4),\n",
       " (113, -4),\n",
       " (106, -4),\n",
       " (165, -4),\n",
       " (65, -4),\n",
       " (222, -4)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743a3e1e-a190-4bda-b823-5e1092cd0ba0",
   "metadata": {},
   "source": [
    "# No causal selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b3228da-3b81-4dc1-a25a-33e5972e05e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = np.concatenate([np.asarray(Xnorml['train'][key].dropna()[target]) for key in Xnorml['train'].keys()],axis=0)\n",
    "Xtrain = np.concatenate([np.asarray(Xnorml['train'][key].dropna().drop(columns=[target])) for key in Xnorml['train'].keys()],axis=0)\n",
    "yvalid = np.concatenate([np.asarray(Xnorml['valid'][key].dropna()[target]) for key in Xnorml['valid'].keys()],axis=0)\n",
    "Xvalid = np.concatenate([np.asarray(Xnorml['valid'][key].dropna().drop(columns=[target])) for key in Xnorml['valid'].keys()],axis=0)\n",
    "ytest = np.concatenate([np.asarray(Xnorml['test'][key].dropna()[target]) for key in Xnorml['test'].keys()],axis=0)\n",
    "Xtest = np.concatenate([np.asarray(Xnorml['test'][key].dropna().drop(columns=[target])) for key in Xnorml['test'].keys()],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cdea2b7-988e-4212-8f30-1c6e7639bfb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3190335148694612, 0.25704625858822294, 0.3399466154523325)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xnorml_nocausal = {'train': Xtrain, 'valid': Xvalid, 'test': Xtest}\n",
    "y = {'train': ytrain, 'valid': yvalid, 'test': ytest}\n",
    "regr = train_baseline.train_baseline_MLR(Xnorml_nocausal,y)\n",
    "\n",
    "MLR_scoreboard = performance_scores.scoreboard(regr).store_scores(Xnorml_nocausal,y)\n",
    "MLR_scoreboard['train']['r2'],MLR_scoreboard['valid']['r2'],MLR_scoreboard['test']['r2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "973ddbfb-6059-4778-b521-db33031d2a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1758, 254)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xvalid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41c3745-e315-4053-a0c7-79263e25a02b",
   "metadata": {},
   "source": [
    "# With causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1bed94ae-1b49-4228-a5b6-ebc75ccbaf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_causal(PC1_results=None,Xnorml=None):\n",
    "    causal_predictor_list = [var_names[i] for i in [obj[0] for obj in PC1_results[0]]]\n",
    "    while target in causal_predictor_list: \n",
    "        causal_predictor_list.remove(target)\n",
    "        \n",
    "    Xtrain_causal = np.concatenate([np.asarray(Xnorml['train'][key].dropna()[causal_predictor_list]) for key in Xnorml['train'].keys()],axis=0)\n",
    "    Xvalid_causal = np.concatenate([np.asarray(Xnorml['valid'][key].dropna()[causal_predictor_list]) for key in Xnorml['valid'].keys()],axis=0)\n",
    "    Xtest_causal = np.concatenate([np.asarray(Xnorml['test'][key].dropna()[causal_predictor_list]) for key in Xnorml['test'].keys()],axis=0)\n",
    "    \n",
    "    Xnorml_causal = {'train': Xtrain_causal, 'valid': Xvalid_causal, 'test': Xtest_causal}\n",
    "    regr_causal = train_baseline.train_baseline_MLR(Xnorml_causal,y)\n",
    "    return performance_scores.scoreboard(regr_causal).store_scores(Xnorml_causal,y),Xnorml_causal,regr_causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "31ef9317-7df6-4abb-b7fe-bb5381694b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores,Xs,regr = [],[],[]\n",
    "for obj in results:\n",
    "    score,X,regrz = benchmark_causal(PC1_results=obj,Xnorml=Xnorml)\n",
    "    scores.append(score)\n",
    "    Xs.append(X)\n",
    "    regr.append(regrz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d6650238-2859-4ded-a84a-065f08308b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.22758870781273133], [0.2620004791744047], [0.24884417564138772])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[obj['train']['r2'] for obj in scores],[obj['valid']['r2'] for obj in scores],[obj['test']['r2'] for obj in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c9510fcb-aba6-4de9-8e10-02528a71f489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.2345899268984507], [0.2580179531419897], [0.25898521559471577])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[obj['train']['r2'] for obj in scores],[obj['valid']['r2'] for obj in scores],[obj['test']['r2'] for obj in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58998935-a5f4-4218-86f6-f3b4b4348643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
